{
  "project": {
    "name": "REDLINE",
    "version": "10.3",
    "date": "2025-05-14",
    "author": "Bret Kuhne",
    "updated_by": "(Bret Kuhne)"
  },
  "introduction": {
    "purpose": "Defines the architecture and implementation of the REDLINE module, a standalone application for loading, storing, preprocessing, and converting stock market data with a Tkinter GUI. It runs in a Docker or Podman container based on Ubuntu 24.04 LTS, ensuring cross-platform compatibility without external environments like Conda.",
    "scope": {
      "functionality": [
        "Load stock market data from files (CSV, JSON, DuckDB, Parquet, Polars, Keras)",
        "Store data in a shared DuckDB database (redline_data.duckdb) with format metadata",
        "Preprocess data for machine learning (ML) and reinforcement learning (RL) models",
        "Convert data between supported formats (CSV, JSON, DuckDB, PyArrow, Polars, Keras)",
        "Provide a Tkinter GUI with tabs for data loading, format conversion, and data viewing"
      ],
      "containerization": [
        "Runs in a Docker or Podman container using Ubuntu 24.04 LTS",
        "Installs dependencies via apt and pip within the container",
        "Persists redline_data.duckdb using a volume mount"
      ],
      "standalone_operation": [
        "Operates independently, with outputs stored in redline_data.duckdb for use by other systems (e.g., ML, RL, or Trading Simulators)",
        "No dependency on external modules or API credentials"
      ]
    },
    "dependencies": {
      "external_libraries": [
        "pandas",
        "sqlalchemy",
        "duckdb",
        "configparser",
        "tkinter",
        "ttk",
        "pyarrow",
        "polars",
        "tensorflow"
      ],
      "system_dependencies": [
        "python3",
        "python3-pip",
        "python3-tk",
        "libpq-dev"
      ],
      "other_modules": "None; outputs to redline_data.duckdb for external systems"
    }
  },
  "system_design": {
    "architecture": {
      "description": "REDLINE operates as a standalone application within a Docker or Podman container based on Ubuntu 24.04 LTS. It initializes a Tkinter GUI and manages data operations via a shared DuckDB database.",
      "diagram": "[User] ↔ [Docker/Podman Container: Ubuntu 24.04 LTS] → [Tkinter GUI (StockAnalyzerGUI: ttk.Notebook)] → [DataLoader: Load/Convert/Preprocess Data] → [DatabaseConnector: Read/Write redline_data.duckdb] → [DataAdapter: Preprocess for ML/RL] → [Output: redline_data.duckdb (tickers_data, preprocessed)]"
    },
    "module_descriptions": {
      "redline_module": {
        "purpose": "Handles loading, storing, preprocessing, and format conversion of stock market data with a Tkinter GUI",
        "inputs": [
          "File paths (CSV, JSON, DuckDB, Parquet)",
          "Configuration (data_config.ini)"
        ],
        "outputs": [
          "DuckDB tables in redline_data.duckdb",
          "Preprocessed data for ML/RL models"
        ],
        "methods": [
          {
            "name": "run",
            "parameters": [{"name": "task", "type": "str"}],
            "description": "Entry point for tasks (load, convert, preprocess)"
          },
          {
            "name": "initialize_gui",
            "description": "Sets up Tkinter GUI with tabs"
          }
        ],
        "configuration": {
          "file": "data_config.ini",
          "content": "[Data]\ndb_path = /app/redline_data.duckdb\ncsv_dir = /app/data\njson_dir = /app/data/json\nparquet_dir = /app/data/parquet"
        },
        "error_handling": [
          "Logs to redline.log using logging",
          "Displays GUI errors via tkinter.messagebox"
        ]
      }
    },
    "gui_design": {
      "name": "Redline Conversion Utility",
      "framework": "Tkinter (via python3-tk in container)",
      "components": {
        "notebook": {
          "type": "ttk.Notebook",
          "tabs": [
            {
              "name": "Data Loader",
              "widgets": [
                {"type": "Label", "text": "Select Input Files"},
                {"type": "Listbox", "name": "input_listbox", "selectmode": "multiple", "width": 50},
                {"type": "Combobox", "name": "input_format", "values": ["csv", "json", "duckdb", "pyarrow", "polars", "keras"], "label": "Input Format"},
                {"type": "Combobox", "name": "output_format", "values": ["csv", "json", "duckdb", "pyarrow", "polars", "keras"], "label": "Output Format"},
                {"type": "Button", "text": "Browse Files", "command": "browse_files"},
                {"type": "Button", "text": "Load and Convert", "command": "load_and_convert"}
              ],
              "functionality": {
                "browse_files": {
                  "description": "Opens filedialog.askopenfilenames to select files (CSV, JSON, DuckDB, Parquet, Keras .h5)",
                  "filetypes": [
                    {"name": "CSV Files", "extension": "*.csv"},
                    {"name": "JSON Files", "extension": "*.json"},
                    {"name": "DuckDB Files", "extension": "*.duckdb"},
                    {"name": "Parquet Files", "extension": "*.parquet"},
                    {"name": "Keras Models", "extension": "*.h5"}
                  ]
                },
                "load_and_convert": {
                  "description": "Validates files, loads via DataLoader.load_data, converts via DataLoader.convert_format, saves to redline_data.duckdb via DatabaseConnector.write_shared_data, shows success/error",
                  "actions": [
                    "Validate input files",
                    "Load data",
                    "Convert format",
                    "Save to redline_data.duckdb",
                    "Show message"
                  ]
                }
              }
            },
            {
              "name": "Data View",
              "widgets": [
                {"type": "Treeview", "name": "data_tree", "columns": ["Ticker", "Date", "Close", "Format"], "show": "headings"},
                {"type": "Button", "text": "Refresh Data", "command": "refresh_data"}
              ],
              "functionality": {
                "refresh_data": {
                  "description": "Queries redline_data.duckdb via DatabaseConnector.read_shared_data, populates Treeview with ticker data",
                  "actions": [
                    "Clear Treeview items",
                    "Read data from redline_data.duckdb",
                    "Populate Treeview"
                  ]
                }
              }
            }
          ]
        }
      }
    },
    "data_flow": [
      {
        "step": "GUI Select Files",
        "description": "User selects files and formats in Data Loader tab",
        "next": "DataLoader Load Data"
      },
      {
        "step": "DataLoader Load Data",
        "description": "Loads data in specified format",
        "next": "DataLoader Convert Format"
      },
      {
        "step": "DataLoader Convert Format",
        "description": "Converts to user-specified output format",
        "next": "DatabaseConnector Save"
      },
      {
        "step": "DatabaseConnector Save",
        "description": "Saves to redline_data.duckdb:tickers_data with format metadata",
        "next": "DataAdapter Preprocess"
      },
      {
        "step": "DataAdapter Preprocess",
        "description": "Preprocesses for ML/RL",
        "next": "DatabaseConnector Save Preprocessed"
      },
      {
        "step": "DatabaseConnector Save Preprocessed",
        "description": "Saves preprocessed data to redline_data.duckdb:preprocessed",
        "next": "GUI Display"
      },
      {
        "step": "GUI Display",
        "description": "Displays data and metadata in Data View tab"
      }
    ],
    "database": {
      "name": "redline_data.duckdb",
      "location": "/app/redline_data.duckdb (container volume)",
      "schema": [
        {
          "table": "tickers_data",
          "columns": [
            {"name": "ticker", "type": "VARCHAR"},
            {"name": "table_name", "type": "VARCHAR"},
            {"name": "fields", "type": "VARCHAR[]"},
            {"name": "data_path", "type": "VARCHAR"},
            {"name": "timestamp", "type": "DATETIME"},
            {"name": "env_name", "type": "VARCHAR"},
            {"name": "env_status", "type": "VARCHAR"},
            {"name": "row_count", "type": "INTEGER"},
            {"name": "format", "type": "VARCHAR", "values": ["pandas", "polars", "pyarrow", "json", "keras"]}
          ],
          "indexes": [
            {"name": "idx_ticker", "column": "ticker"},
            {"name": "idx_format", "column": "format"}
          ]
        },
        {
          "table": "preprocessed",
          "columns": [
            {"name": "ticker", "type": "VARCHAR"},
            {"name": "sequence", "type": "BLOB"},
            {"name": "features", "type": "VARCHAR[]"},
            {"name": "timestamp", "type": "DATETIME"},
            {"name": "format", "type": "VARCHAR", "values": ["numpy", "tensorflow"]}
          ],
          "indexes": [
            {"name": "idx_ticker_preprocessed", "column": "ticker"}
          ]
        }
      ]
    },
    "classes": [
      {
        "name": "DataLoader",
        "purpose": "Loads, converts, and stores stock data from various file formats",
        "inputs": ["File paths", "data_config.ini"],
        "outputs": "DuckDB tables in redline_data.duckdb",
        "methods": [
          {
            "name": "load_data",
            "parameters": [
              {"name": "file_paths", "type": "list[str]"},
              {"name": "format", "type": "str"}
            ],
            "description": "Loads data, returns list[Union[pd.DataFrame, pl.DataFrame, pa.Table]]"
          },
          {
            "name": "validate_data",
            "parameters": [
              {"name": "file_path", "type": "str"},
              {"name": "format", "type": "str"}
            ],
            "description": "Checks for required columns (ticker, timestamp, close), returns bool"
          },
          {
            "name": "convert_format",
            "parameters": [
              {"name": "data", "type": "Union[pd.DataFrame, pl.DataFrame, pa.Table]"},
              {"name": "from_format", "type": "str"},
              {"name": "to_format", "type": "str"}
            ],
            "description": "Converts between formats, returns converted data"
          },
          {
            "name": "save_to_shared",
            "parameters": [
              {"name": "table", "type": "str"},
              {"name": "data", "type": "Union[pd.DataFrame, pl.DataFrame, pa.Table]"},
              {"name": "format", "type": "str"}
            ],
            "description": "Writes to redline_data.duckdb"
          }
        ]
      },
      {
        "name": "DatabaseConnector",
        "purpose": "Manages DuckDB connections",
        "inputs": ["DB path", "Table name", "Format"],
        "outputs": ["Data in specified format", "Metadata"],
        "methods": [
          {
            "name": "create_connection",
            "parameters": [{"name": "db_path", "type": "str"}],
            "description": "Creates SQLAlchemy engine, returns engine"
          },
          {
            "name": "read_shared_data",
            "parameters": [
              {"name": "table", "type": "str"},
              {"name": "format", "type": "str"}
            ],
            "description": "Reads data, returns Union[pd.DataFrame, pl.DataFrame, pa.Table]"
          },
          {
            "name": "write_shared_data",
            "parameters": [
              {"name": "table", "type": "str"},
              {"name": "data", "type": "Union[pd.DataFrame, pl.DataFrame, pa.Table]"},
              {"name": "format", "type": "str"}
            ],
            "description": "Writes data with metadata"
          }
        ]
      },
      {
        "name": "DataAdapter",
        "purpose": "Preprocesses data for ML/RL models",
        "inputs": ["Data", "Config parameters (sequence length, features)"],
        "outputs": ["Scaled sequences", "RL states", "Keras datasets"],
        "methods": [
          {
            "name": "prepare_training_data",
            "parameters": [
              {"name": "data", "type": "Union[list[pd.DataFrame], list[pl.DataFrame], list[pa.Table]]"},
              {"name": "format", "type": "str"}
            ],
            "description": "Creates ML sequences, returns Union[list[np.ndarray], tf.data.Dataset]"
          },
          {
            "name": "prepare_rl_state",
            "parameters": [
              {"name": "data", "type": "Union[pd.DataFrame, pl.DataFrame, pa.Table]"},
              {"name": "portfolio", "type": "dict"},
              {"name": "format", "type": "str"}
            ],
            "description": "Generates RL states, returns Union[np.ndarray, tf.Tensor]"
          },
          {
            "name": "summarize_preprocessed",
            "parameters": [
              {"name": "data", "type": "Union[list[np.ndarray], tf.data.Dataset]"},
              {"name": "format", "type": "str"}
            ],
            "description": "Summarizes data for GUI, returns dict"
          }
        ]
      }
    ]
  },
  "implementation_details": {
    "containerization": {
      "environment": "Ubuntu 24.04 LTS container (Docker or Podman)",
      "dockerfile": "FROM ubuntu:24.04\nRUN apt-get update && apt-get install -y \\\n    python3 \\\n    python3-pip \\\n    python3-tk \\\n    libpq-dev \\\n    && rm -rf /var/lib/apt/lists/*\nRUN pip3 install pandas sqlalchemy duckdb configparser pyarrow polars tensorflow\nWORKDIR /app\nCOPY data_module.py data_config.ini ./\nVOLUME /app/data /app/redline_data.duckdb\nCMD [\"python3\", \"-m\", \"data_module\", \"--task=load\"]",
      "execution": [
        "Build: docker build -t redline . or podman build -t redline .",
        "Run: docker run --rm -v $(pwd)/data:/app/data -v $(pwd)/redline_data.duckdb:/app/redline_data.duckdb redline",
        "GUI: Tkinter requires X11 forwarding or VNC; alternatively, run GUI locally"
      ]
    },
    "standalone_operation": {
      "execution": "python3 -m data_module --task=<task>",
      "tasks": [
        {"name": "load", "description": "Ingests data into redline_data.duckdb"},
        {"name": "convert", "description": "Converts between formats"},
        {"name": "preprocess", "description": "Generates ML/RL-ready data"}
      ],
      "fallback": "Uses cached JSON/Parquet data if file access fails"
    },
    "testing": {
      "unit_tests": [
        "Test DataLoader.load_data for all formats",
        "Test DataLoader.convert_format for all format pairs",
        "Test DatabaseConnector.write_shared_data for dynamic tables",
        "Test GUI load_and_convert with mock data"
      ],
      "integration_tests": [
        "Verify data in redline_data.duckdb with format metadata",
        "Ensure preprocessed data displays in GUI",
        "Test round-trip conversion (CSV → Polars → PyArrow → DuckDB)"
      ],
      "mock_testing": "Use unittest.mock for file access simulation"
    }
  },
  "assumptions": [
    "redline_data.duckdb exists or can be created in container volume",
    "Input files have required columns (ticker, timestamp, close)",
    "Docker or Podman installed on host",
    "X11 forwarding or VNC for GUI (if run in container)"
  ],
  "future_enhancements": [
    "Support API data sources (e.g., Stooq, Alpaca) with credential management via a configuration tab",
    "Support drag-and-drop file inputs",
    "Add progress bar for file loading/conversion",
    "Enable data export (CSV, JSON, Parquet)",
    "Support additional data sources (e.g., Bloomberg)",
    "Implement data caching for offline operation"
  ],
  "sample_implementation": {
    "data_module_py": "#!/usr/bin/env python3\n\"\"\"REDLINE module for stock market data management in a Docker/Podman container.\"\"\"\n\nimport logging\nimport sys\nimport configparser\nimport pandas as pd\nimport polars as pl\nimport pyarrow as pa\nimport duckdb\nimport sqlalchemy\nfrom sqlalchemy import create_engine\nimport tensorflow as tf\nimport tkinter as tk\nfrom tkinter import ttk, filedialog, messagebox\nfrom typing import Union, List, Dict\nimport argparse\n\n# Configure logging\nlogging.basicConfig(filename='redline.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\nclass DataLoader:\n    def __init__(self, config_path: str = 'data_config.ini'):\n        self.config = configparser.ConfigParser()\n        self.config.read(config_path)\n        self.db_path = self.config['Data'].get('db_path', '/app/redline_data.duckdb')\n        self.csv_dir = self.config['Data'].get('csv_dir', '/app/data')\n        self.json_dir = self.config['Data'].get('json_dir', '/app/data/json')\n        self.parquet_dir = self.config['Data'].get('parquet_dir', '/app/data/parquet')\n\n    def load_data(self, file_paths: List[str], format: str) -> List[Union[pd.DataFrame, pl.DataFrame, pa.Table]]:\n        data = []\n        for path in file_paths:\n            if not self.validate_data(path, format):\n                raise ValueError(f\"Invalid data in {path} for format {format}\")\n            try:\n                if format == 'csv':\n                    data.append(pd.read_csv(path))\n                elif format == 'json':\n                    data.append(pd.read_json(path))\n                elif format == 'duckdb':\n                    conn = duckdb.connect(path)\n                    data.append(conn.execute(\"SELECT * FROM tickers_data\").fetchdf())\n                    conn.close()\n                elif format == 'pyarrow':\n                    data.append(pa.parquet.read_table(path))\n                elif format == 'polars':\n                    data.append(pl.read_parquet(path))\n                elif format == 'keras':\n                    data.append(tf.keras.models.load_model(path))\n                logging.info(f\"Loaded {path} as {format}\")\n            except Exception as e:\n                logging.error(f\"Failed to load {path}: {str(e)}\")\n                raise\n        return data\n\n    def validate_data(self, file_path: str, format: str) -> bool:\n        try:\n            if format in ['csv', 'json']:\n                df = pd.read_csv(file_path) if format == 'csv' else pd.read_json(file_path)\n                required = ['ticker', 'timestamp', 'close']\n                return all(col in df.columns for col in required)\n            return True  # Simplified for other formats\n        except Exception as e:\n            logging.error(f\"Validation failed for {file_path}: {str(e)}\")\n            return False\n\n    def convert_format(self, data: Union[pd.DataFrame, pl.DataFrame, pa.Table], from_format: str, to_format: str) -> Union[pd.DataFrame, pl.DataFrame, pa.Table, dict]:\n        if from_format == to_format:\n            return data\n        if isinstance(data, list):\n            return [self.convert_format(d, from_format, to_format) for d in data]\n        try:\n            if from_format == 'pandas':\n                if to_format == 'polars':\n                    return pl.from_pandas(data)\n                elif to_format == 'pyarrow':\n                    return pa.Table.from_pandas(data)\n            elif from_format == 'polars':\n                if to_format == 'pandas':\n                    return data.to_pandas()\n                elif to_format == 'pyarrow':\n                    return data.to_arrow()\n            elif from_format == 'pyarrow':\n                if to_format == 'pandas':\n                    return data.to_pandas()\n                elif to_format == 'polars':\n                    return pl.from_arrow(data)\n            logging.info(f\"Converted from {from_format} to {to_format}\")\n            return data\n        except Exception as e:\n            logging.error(f\"Conversion failed from {from_format} to {to_format}: {str(e)}\")\n            raise\n\n    def save_to_shared(self, table: str, data: Union[pd.DataFrame, pl.DataFrame, pa.Table], format: str) -> None:\n        try:\n            conn = duckdb.connect(self.db_path)\n            if isinstance(data, pl.DataFrame):\n                data = data.to_pandas()\n            elif isinstance(data, pa.Table):\n                data = data.to_pandas()\n            conn.execute(f\"CREATE TABLE IF NOT EXISTS {table} (ticker VARCHAR, table_name VARCHAR, fields VARCHAR[], data_path VARCHAR, timestamp DATETIME, env_name VARCHAR, env_status VARCHAR, row_count INTEGER, format VARCHAR)\")\n            data['format'] = format\n            data.to_sql(table, create_engine(f'duckdb:///{self.db_path}'), if_exists='append', index=False)\n            conn.close()\n            logging.info(f\"Saved data to {table} in format {format}\")\n        except Exception as e:\n            logging.error(f\"Failed to save to {table}: {str(e)}\")\n            raise\n\nclass DatabaseConnector:\n    def __init__(self, db_path: str = '/app/redline_data.duckdb'):\n        self.db_path = db_path\n\n    def create_connection(self, db_path: str) -> sqlalchemy.engine.Engine:\n        return create_engine(f'duckdb:///{db_path}')\n\n    def read_shared_data(self, table: str, format: str) -> Union[pd.DataFrame, pl.DataFrame, pa.Table]:\n        try:\n            conn = duckdb.connect(self.db_path)\n            df = conn.execute(f\"SELECT ticker, timestamp, close, format FROM {table}\").fetchdf()\n            conn.close()\n            if format == 'polars':\n                return pl.from_pandas(df)\n            elif format == 'pyarrow':\n                return pa.Table.from_pandas(df)\n            return df\n        except Exception as e:\n            logging.error(f\"Failed to read from {table}: {str(e)}\")\n            raise\n\n    def write_shared_data(self, table: str, data: Union[pd.DataFrame, pl.DataFrame, pa.Table], format: str) -> None:\n        try:\n            conn = duckdb.connect(self.db_path)\n            if isinstance(data, pl.DataFrame):\n                data = data.to_pandas()\n            elif isinstance(data, pa.Table):\n                data = data.to_pandas()\n            data['format'] = format\n            data.to_sql(table, create_engine(f'duckdb:///{self.db_path}'), if_exists='append', index=False)\n            conn.close()\n            logging.info(f\"Wrote data to {table} in format {format}\")\n        except Exception as e:\n            logging.error(f\"Failed to write to {table}: {str(e)}\")\n            raise\n\nclass DataAdapter:\n    def prepare_training_data(self, data: Union[List[pd.DataFrame], List[pl.DataFrame], List[pa.Table]], format: str) -> Union[List[np.ndarray], tf.data.Dataset]:\n        try:\n            if isinstance(data, list) and data:\n                if format == 'numpy':\n                    return [d.to_numpy() for d in data if isinstance(d, (pd.DataFrame, pl.DataFrame))]\n                elif format == 'tensorflow':\n                    return tf.data.Dataset.from_tensor_slices([d.to_numpy() for d in data if isinstance(d, (pd.DataFrame, pl.DataFrame))])\n            return []\n        except Exception as e:\n            logging.error(f\"Failed to prepare training data: {str(e)}\")\n            raise\n\n    def prepare_rl_state(self, data: Union[pd.DataFrame, pl.DataFrame, pa.Table], portfolio: Dict, format: str) -> Union[np.ndarray, tf.Tensor]:\n        try:\n            if isinstance(data, (pl.DataFrame, pa.Table)):\n                data = data.to_pandas()\n            state = data[['close']].to_numpy()\n            if format == 'tensorflow':\n                return tf.convert_to_tensor(state, dtype=tf.float32)\n            return state\n        except Exception as e:\n            logging.error(f\"Failed to prepare RL state: {str(e)}\")\n            raise\n\n    def summarize_preprocessed(self, data: Union[List[np.ndarray], tf.data.Dataset], format: str) -> Dict:\n        try:\n            return {'format': format, 'size': len(data)}\n        except Exception as e:\n            logging.error(f\"Failed to summarize preprocessed data: {str(e)}\")\n            raise\n\nclass StockAnalyzerGUI:\n    def __init__(self, root: tk.Tk, loader: DataLoader, connector: DatabaseConnector):\n        self.root = root\n        self.root.title(\"REDLINE Stock Analyzer\")\n        self.loader = loader\n        self.connector = connector\n        self.notebook = ttk.Notebook(self.root)\n        self.notebook.pack(fill='both', expand=True)\n        self.setup_tabs()\n\n    def setup_tabs(self):\n        # Data Loader Tab\n        loader_frame = ttk.Frame(self.notebook)\n        self.notebook.add(loader_frame, text='Data Loader')\n        ttk.Label(loader_frame, text=\"Select Input Files:\").pack()\n        self.input_listbox = tk.Listbox(loader_frame, selectmode='multiple', width=50)\n        self.input_listbox.pack()\n        ttk.Label(loader_frame, text=\"Input Format\").pack()\n        self.input_format = ttk.Combobox(loader_frame, values=['csv', 'json', 'duckdb', 'pyarrow', 'polars', 'keras'])\n        self.input_format.pack()\n        ttk.Label(loader_frame, text=\"Output Format\").pack()\n        self.output_format = ttk.Combobox(loader_frame, values=['csv', 'json', 'duckdb', 'pyarrow', 'polars', 'keras'])\n        self.output_format.pack()\n        ttk.Button(loader_frame, text=\"Browse Files\", command=self.browse_files).pack()\n        ttk.Button(loader_frame, text=\"Load and Convert\", command=self.load_and_convert).pack()\n\n        # Data View Tab\n        view_frame = ttk.Frame(self.notebook)\n        self.notebook.add(view_frame, text='Data View')\n        self.data_tree = ttk.Treeview(view_frame, columns=['Ticker', 'Date', 'Close', 'Format'], show='headings')\n        self.data_tree.heading('Ticker', text='Ticker')\n        self.data_tree.heading('Date', text='Date')\n        self.data_tree.heading('Close', text='Close')\n        self.data_tree.heading('Format', text='Format')\n        self.data_tree.pack(fill='both', expand=True)\n        ttk.Button(view_frame, text=\"Refresh Data\", command=self.refresh_data).pack()\n\n    def browse_files(self):\n        filetypes = [\n            ('CSV Files', '*.csv'),\n            ('JSON Files', '*.json'),\n            ('DuckDB Files', '*.duckdb'),\n            ('Parquet Files', '*.parquet'),\n            ('Keras Models', '*.h5')\n        ]\n        files = filedialog.askopenfilenames(filetypes=filetypes)\n        self.input_listbox.delete(0, tk.END)\n        for file in files:\n            self.input_listbox.insert(tk.END, file)\n\n    def load_and_convert(self):\n        try:\n            files = self.input_listbox.get(0, tk.END)\n            input_format = self.input_format.get()\n            output_format = self.output_format.get()\n            if not files or not input_format or not output_format:\n                messagebox.showerror(\"Error\", \"Select files and formats\")\n                return\n            data = self.loader.load_data(list(files), input_format)\n            converted = self.loader.convert_format(data, input_format, output_format)\n            self.loader.save_to_shared('tickers_data', converted[0] if isinstance(converted, list) else converted, output_format)\n            messagebox.showinfo(\"Success\", \"Data loaded and converted\")\n        except Exception as e:\n            logging.error(f\"Load and convert failed: {str(e)}\")\n            messagebox.showerror(\"Error\", f\"Load and convert failed: {str(e)}\")\n\n    def refresh_data(self):\n        try:\n            for item in self.data_tree.get_children():\n                self.data_tree.delete(item)\n            data = self.connector.read_shared_data('tickers_data', 'pandas')\n            for _, row in data.iterrows():\n                self.data_tree.insert('', 'end', values=(row['ticker'], row['timestamp'], row['close'], row['format']))\n        except Exception as e:\n            logging.error(f\"Refresh data failed: {str(e)}\")\n            messagebox.showerror(\"Error\", f\"Refresh data failed: {str(e)}\")\n\ndef run(task: str = 'gui'):\n    loader = DataLoader()\n    connector = DatabaseConnector(loader.db_path)\n    if task == 'gui':\n        root = tk.Tk()\n        app = StockAnalyzerGUI(root, loader, connector)\n        root.mainloop()\n    elif task in ['load', 'convert', 'preprocess']:\n        # Example for load task\n        if task == 'load':\n            data = loader.load_data([f\"{loader.csv_dir}/sample.csv\"], 'csv')\n            loader.save_to_shared('tickers_data', data[0], 'pandas')\n        logging.info(f\"Completed task: {task}\")\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--task', default='gui', choices=['gui', 'load', 'convert', 'preprocess'])\n    args = parser.parse_args()\n    run(args.task)",
    "data_config_ini": "[Data]\ndb_path = /app/redline_data.duckdb\ncsv_dir = /app/data\njson_dir = /app/data/json\nparquet_dir = /app/data/parquet"
  },
  "git_integration": {
    "repository": "data_manager (public GitHub repository)",
    "push_commands": [
      "cd ~/path/to/data_manager",
      "git add data_module.py data_config.ini Dockerfile",
      "git commit -m \"Update REDLINE module with redline_data.duckdb\"",
      "git push origin main --force"
    ],
    "gitignore": "redline_data.duckdb\n*.log\ndata/"
  },
  "deployment": {
    "docker": {
      "build": "docker build -t redline .",
      "run": "docker run --rm -v $(pwd)/data:/app/data -v $(pwd)/redline_data.duckdb:/app/redline_data.duckdb redline",
      "gui_note": "For containerized GUI, set up X11 forwarding: xhost +local:docker; docker run --rm -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix -v $(pwd)/data:/app/data -v $(pwd)/redline_data.duckdb:/app/redline_data.duckdb redline"
    },
    "podman": {
      "build": "podman build -t redline .",
      "run": "podman run --rm -v $(pwd)/data:/app/data:Z -v $(pwd)/redline_data.duckdb:/app/redline_data.duckdb:Z redline"
    }
  },
  "version": "2.0",
  "description": "Redline Conversion Utility - Modular Workflow Template",
  "workflow": {
    "input_files": [
      {
        "path": "data/input1.csv",
        "format": "csv"
      },
      {
        "path": "data/input2.json",
        "format": "json"
      }
    ],
    "output": {
      "path": "data/output/merged.parquet",
      "format": "parquet"
    },
    "file_format_module": {
      "supported_formats": ["csv", "txt", "json", "duckdb", "parquet", "feather", "tfrecord", "hdf5", "npy", "pickle", "arrow", "libsvm", "coco", "yolo"]
    },
    "schema_module": {
      "infer_schema": true,
      "validate_schema": true,
      "custom_schema": {
        "columns": [
          {"name": "ticker", "type": "string"},
          {"name": "timestamp", "type": "string"},
          {"name": "open", "type": "float"},
          {"name": "high", "type": "float"},
          {"name": "low", "type": "float"},
          {"name": "close", "type": "float"},
          {"name": "vol", "type": "float"},
          {"name": "openint", "type": "float"},
          {"name": "format", "type": "string"}
        ]
      }
    },
    "preprocessing_pipeline": [
      {"step": "impute_missing", "strategy": "mean"},
      {"step": "scale", "method": "minmax", "columns": ["open", "high", "low", "close", "vol", "openint"]},
      {"step": "encode_categorical", "columns": ["ticker"]},
      {"step": "feature_engineering", "operations": ["moving_average", "volatility"]}
    ],
    "ml_ai_integration": {
      "export_format": "tfrecord",
      "framework": "tensorflow",
      "validate_model_io": true,
      "model_path": "models/my_model.h5"
    },
    "batch_automation": {
      "enabled": true,
      "watch_folder": "data/incoming/",
      "schedule": "0 2 * * *",  // daily at 2am
      "on_new_file": "run_workflow"
    },
    "visualization": {
      "show_stats": true,
      "visualize_sample": ["histogram", "missing_heatmap"],
      "detect_drift": {
        "enabled": true,
        "reference_file": "data/reference.parquet"
      }
    },
    "performance": {
      "parallel": true,
      "backend": "dask",
      "chunk_size": 100000
    },
    "documentation": {
      "show_help": true,
      "run_tutorial": "basic_conversion"
    },
    "logging": {
      "level": "info",
      "log_file": "redline.log"
    }
  },
  "plugins": [
    "custom_format_plugin.py",
    "advanced_visualization_plugin.py"
  ],
  "flexibility_features": {
    "plugin_system": {
      "enabled": true,
      "plugin_directory": "app/plugins/",
      "plugin_api_doc": "See docs/plugins.md for how to write and register plugins."
    },
    "workflow_engine": {
      "config_format": "yaml|json",
      "conditional_logic": true,
      "example": {
        "if": "missing_values > 10%",
        "then": "impute_missing",
        "else": "drop_rows"
      }
    },
    "cli_api_jupyter": {
      "cli_enabled": true,
      "api_enabled": true,
      "jupyter_integration": true,
      "api_type": "REST (FastAPI)",
      "jupyter_widgets": true
    },
    "advanced_data_sources": {
      "databases": ["postgres", "mysql", "sqlite"],
      "cloud_storage": ["s3", "gcs", "azure_blob"],
      "api_ingestion": ["yahoo_finance", "alpaca", "stooq"]
    },
    "user_profiles_presets": {
      "profiles_enabled": true,
      "presets": ["tensorflow_image_pipeline", "tabular_ml_clean"]
    },
    "validation_reporting": {
      "schema_evolution": true,
      "data_quality_checks": ["missing", "outliers", "duplicates", "type_mismatch"],
      "reporting": ["html", "pdf"]
    },
    "interactive_dashboards": {
      "enabled": true,
      "frameworks": ["plotly_dash", "streamlit", "jupyter_widgets"]
    },
    "distributed_streaming": {
      "distributed_backends": ["dask", "ray", "spark"],
      "streaming_enabled": true
    },
    "resource_monitoring": {
      "enabled": true,
      "metrics": ["cpu", "memory", "progress"]
    },
    "centralized_error_handling": {
      "enabled": true,
      "retry_resume": true,
      "error_log_file": "redline_errors.log"
    },
    "in_app_docs_tutorials": {
      "contextual_help": true,
      "tutorials": ["basic_conversion", "ml_pipeline", "custom_plugin"]
    },
    "security_compliance": {
      "sandbox_plugins": true,
      "audit_trail": true,
      "credential_management": "secure_store"
    },
    "testing_ci": {
      "test_harness": true,
      "ci_enabled": true,
      "ci_platform": "github_actions"
    }
  }
}
